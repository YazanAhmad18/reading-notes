# at the beginning i knew what i am going to face at this level:
* You’ll be pushed mentally, having to think your way through problems that you had only even heard about a few hours beforehand.
* You’ll have to forge your own path toward a solution.
* You’ll have to research for hours on end, fleshing out the skeleton that we discuss in class, piling information on top of application so that you can reach your goals.
* You’ll be pushed emotionally, constantly confronted with your own lack of understanding of a topic.
* You’ll Constantly be having to figure out how to collaborate with new people and deal with their (and your own) emotional quirks.
* You’ll Constantly be thrust far outside of your comfort zone with the expectation that you’ll survive for the next push forward.
* You’ll Constantly be dealing with uncertainty of what awaits you at the end of the 10 weeks, and whether or not you’ll make it.
* You’ll be pushed physically, and while sitting in a chair and staring at your screen isn’t the most strenuous exercise in the world, the consecutive hours of it will take its toll.
* You’ll lose sleep, you’ll forget to work out, and you’ll feel exhausted all while being filled with information day after day.

## as i understand the growth its not gonna be easy it will come with pain ,pressure, challenge ,fears .....

## the defetion of Suffering is pain without purpose. Pain with no higher goal. Pain with no dreams, no ambition, no aspiration.

## as i learn i have to ask my self some question  to give my self some feel good why i have this pressure and challenges:

* What’s your perspective?
* Why are you doing this?
* Do you want what comes at the end of this journey?
* Are you doing this for you?



# A beginner's guide to Big O Notation

## Big O notation is used in Computer Science to describe the performance or complexity of an algorithm. Big O specifically describes the worst-case scenario, and can be used to describe the execution time required or the space used 


## types of Big 0 :
* O(1):describes an algorithm that will always execute in the same time (or space) regardless of the size of the input data set.

* O(N)" describes an algorithm whose performance will grow linearly and in direct proportion to the size of the input data set

* O(N²):represents an algorithm whose performance is directly proportional to the square of the size of the input data set. This is common with algorithms that involve nested iterations over the data set.

* O(2^N): denotes an algorithm whose growth doubles with each addition to the input data set. The growth curve of an O(2^N) function is exponential — starting off very shallow, then rising meteorically


# Logarithms used in computer imaging align pixels, organize colors and help computers manipulate photographs for enhancement, merging or comparison

## of these algorithms is: Binary search is a technique used to search sorted data sets. It works by selecting the middle element of the data set, essentially the median, and compares it against a target value. If the values match, it will return success. If the target value is higher than the value of the probe element, it will take the upper half of the data set and perform the same operation against it. Likewise, if the target value is lower than the value of the probe element, it will perform the operation against the lower half. It will continue to halve the data set with each iteration until the value has been found or until it can no longer split the data set.
## This type of algorithm is described as O(log N). The iterative halving of data sets described in the binary search example produces a growth curve that peaks at the beginning and slowly flattens out as the size of the data sets increase e.g. an input data set containing 10 items takes one second to complete, a data set containing 100 items takes two seconds, and a data set containing 1,000 items will take three seconds. Doubling the size of the input data set has little effect on its growth as after a single iteration of the algorithm the data set will be halved and therefore on a par with an input data set half the size. This makes algorithms like binary search extremely efficient when dealing with large data sets.